{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential for modelling\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV, ParameterGrid\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Other packages\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fetch = fetch_ucirepo(id=545)\n",
    "X = fetch.data.features\n",
    "y = fetch.data.targets\n",
    "rice = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Splitting into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=2023)\n",
    "\n",
    "# Encoding Class (Cammeo and Osmancik into 0 and 1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_LE = y.copy()\n",
    "y_LE['Class'] = label_encoder.fit_transform(y_LE['Class'])\n",
    "\n",
    "# Splitting into train and test data with labels encoded (needed for XGBoost)\n",
    "X_train_LE, X_test_LE, y_train_LE, y_test_LE = train_test_split(X, y_LE, test_size = 0.2, random_state=2023)\n",
    "\n",
    "# Normalize the data to use when needed\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train_LE = scaler.fit_transform(X_train_LE)\n",
    "X_test_LE = scaler.transform(X_test_LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling the data and making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting K-Nearest Neighbors...\n",
      "Starting Decision Tree...\n",
      "Starting Random Forest...\n",
      "Starting Support Vector Machines...\n",
      "SVC(C=100, gamma=0.01)\n",
      "Starting Naive Bayes...\n",
      "Accuracy: 92.0%\n",
      "\n",
      "Classification Report\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      Cammeo       0.89      0.92      0.90       308\n",
      "    Osmancik       0.94      0.92      0.93       454\n",
      "\n",
      "    accuracy                           0.92       762\n",
      "   macro avg       0.92      0.92      0.92       762\n",
      "weighted avg       0.92      0.92      0.92       762\n",
      "\n",
      "Confusion Matrix\n",
      " [[283  25]\n",
      " [ 36 418]]\n",
      "Starting Logistic Regression...\n",
      "Starting Stochastic Gradient Descent...\n",
      "Starting XGBoost...\n",
      "Starting Perceptron...\n",
      "Starting Gradient Boosting...\n",
      "Starting AdaBoost...\n",
      "Starting Bagging...\n",
      "Starting Extra Trees...\n",
      "Starting Multi-Layer Perceptron...\n",
      "Starting Gaussian Process...\n",
      "Starting Quadratic Discriminant...\n",
      "Best Parameters for Quadratic Discriminant Analysis: {'tol': 0.0001}\n",
      "Starting CatBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:241: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216 from C header, got 232 from PyObject\n"
     ]
    }
   ],
   "source": [
    "##### K-NEAREST NEIGHBORS\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "print(\"Starting K-Nearest Neighbors...\")\n",
    "\n",
    "KNN_results = []\n",
    "\n",
    "for k in range(1, 51):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train.values.ravel())  # Using ravel() to convert to 1D array\n",
    "    yhat = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    KNN_results.append({'K-value': k, 'Accuracy': accuracy*100})\n",
    "\n",
    "KNN_accuracy_df = pd.DataFrame(KNN_results)\n",
    "\n",
    "# Find index of maximum accuracy\n",
    "KNN_max_index = KNN_accuracy_df['Accuracy'].idxmax()\n",
    "KNN_k_max = int(KNN_accuracy_df.loc[KNN_max_index]['K-value'])\n",
    "KNN_acc_max = KNN_accuracy_df.loc[KNN_max_index]['Accuracy']\n",
    "\n",
    "# Creating the model using the optimum value for K\n",
    "KNN_model = KNeighborsClassifier(n_neighbors = KNN_k_max)\n",
    "KNN_model.fit(X_train, y_train.values.ravel())\n",
    "KNN_pred = KNN_model.predict(X_test)\n",
    "KNN_accuracy = accuracy_score(y_test.values.ravel(), KNN_pred)\n",
    "KNN_cr = classification_report(y_test.values.ravel(), KNN_pred)\n",
    "KNN_cm = confusion_matrix(y_test.values.ravel(), KNN_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### DECISION TREE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "print(\"Starting Decision Tree...\")\n",
    "\n",
    "DT_results = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    clf_tree = DecisionTreeClassifier(criterion=\"entropy\", random_state = 100, max_depth = i)\n",
    "    clf_tree.fit(X_train, y_train.values.ravel())\n",
    "    yhat = clf_tree.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, yhat)\n",
    "    DT_results.append({'Depth': i, 'Accuracy': accuracy*100})\n",
    "    \n",
    "DT_accuracy_df = pd.DataFrame(DT_results)\n",
    "\n",
    "# Find index of maximum accuracy\n",
    "DT_max_index = DT_accuracy_df['Accuracy'].idxmax()\n",
    "DT_depth_max = int(DT_accuracy_df.loc[DT_max_index]['Depth'])\n",
    "DT_acc_max = DT_accuracy_df.loc[DT_max_index]['Accuracy']\n",
    "\n",
    "# Creating the model using the optimum value for Depth\n",
    "DT_model = DecisionTreeClassifier(criterion=\"entropy\", random_state = 100, max_depth = DT_depth_max)\n",
    "DT_model.fit(X_train, y_train.values.ravel())\n",
    "DT_pred = DT_model.predict(X_test)\n",
    "DT_accuracy = accuracy_score(y_test.values.ravel(), DT_pred)\n",
    "DT_cr = classification_report(y_test.values.ravel(), DT_pred)\n",
    "DT_cm = confusion_matrix(y_test.values.ravel(), DT_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### RANDOM FOREST\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"Starting Random Forest...\")\n",
    "\n",
    "max_depth_values = list(range(2, 11))  # Test max_depth from 2 to 10\n",
    "n_estimators_values = list(range(2, 103, 5))  # Test n_estimators from 2 to 102 with step size 5\n",
    "\n",
    "RF_results = []\n",
    "\n",
    "for max_depth in max_depth_values:\n",
    "    for n_estimators in n_estimators_values:\n",
    "        clf_forest = RandomForestClassifier(\n",
    "            n_estimators=n_estimators, \n",
    "            criterion=\"entropy\", \n",
    "            random_state=100, \n",
    "            max_depth=max_depth\n",
    "        )\n",
    "        clf_forest.fit(X_train, y_train.values.ravel())\n",
    "        yhat = clf_forest.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, yhat)\n",
    "        RF_results.append({'Depth': max_depth, 'Estimators': n_estimators, 'Accuracy': accuracy*100})\n",
    "\n",
    "RF_accuracy_df = pd.DataFrame(RF_results)\n",
    "\n",
    "# Find index of maximum accuracy\n",
    "RF_max_index = RF_accuracy_df['Accuracy'].idxmax()\n",
    "RF_depth_max = int(RF_accuracy_df.loc[RF_max_index]['Depth'])\n",
    "RF_estimators_max = int(RF_accuracy_df.loc[RF_max_index]['Estimators'])\n",
    "RF_acc_max = RF_accuracy_df.loc[RF_max_index]['Accuracy']\n",
    "\n",
    "# Creating the model using the optimum value for Depth\n",
    "RF_model = RandomForestClassifier(n_estimators=RF_estimators_max, criterion=\"entropy\", random_state=100, max_depth=RF_depth_max)\n",
    "RF_model.fit(X_train, y_train.values.ravel())\n",
    "RF_pred = RF_model.predict(X_test)\n",
    "RF_accuracy = accuracy_score(y_test.values.ravel(), RF_pred)\n",
    "RF_cr = classification_report(y_test.values.ravel(), RF_pred)\n",
    "RF_cm = confusion_matrix(y_test.values.ravel(), RF_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### SUPPORT VECTOR MACHINES\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "print(\"Starting Support Vector Machines...\")\n",
    "\n",
    "# Grid search for the best parameters\n",
    "SVM_param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "SVM_grid = GridSearchCV(SVC(), SVM_param_grid, refit = True, verbose = 0)\n",
    "SVM_grid.fit(X_train, y_train.values.ravel())\n",
    "print(SVM_grid.best_estimator_)\n",
    "\n",
    "# Creating the model using the best parameters from the Grid Search\n",
    "SVM_pred = SVM_grid.predict(X_test)\n",
    "SVM_accuracy = accuracy_score(y_test.values.ravel(), SVM_pred)\n",
    "SVM_cr = classification_report(y_test.values.ravel(), SVM_pred)\n",
    "SVM_cm = confusion_matrix(y_test.values.ravel(), SVM_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### NAIVE BAYES\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "print(\"Starting Naive Bayes...\")\n",
    "\n",
    "# Unlike the other models, Naive Bayes has no parameters that need tuning\n",
    "NB_model = GaussianNB()\n",
    "NB_model.fit(X_train, y_train.values.ravel())\n",
    "NB_pred = NB_model.predict(X_test)\n",
    "NB_accuracy = accuracy_score(y_test.values.ravel(), NB_pred)\n",
    "NB_cr = classification_report(y_test.values.ravel(), NB_pred)\n",
    "NB_cm = confusion_matrix(y_test.values.ravel(), NB_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### LOGISTIC REGRESSION\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "print(\"Starting Logistic Regression...\")\n",
    "\n",
    "# Define ranges for hyperparameters\n",
    "penalty_values = ['l1', 'l2']\n",
    "C_values = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]  # List of C values to test\n",
    "solver_values = ['liblinear', 'saga', 'sag', 'lbfgs']  # Different solvers supporting both 'l1' and 'l2' penalties\n",
    "\n",
    "LR_results = []\n",
    "\n",
    "for params in ParameterGrid({'penalty': penalty_values, 'C': C_values, 'solver': solver_values}):\n",
    "    if (params['solver'] in ['lbfgs', 'sag']) and params['penalty'] == 'l1':\n",
    "        continue  # Skip 'lbfgs' and 'sag' solvers with 'l1' penalty\n",
    "    clf_lr = LogisticRegression(**params)\n",
    "    clf_lr.fit(X_train, y_train.values.ravel())\n",
    "    yhat = clf_lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test.values.ravel(), yhat)\n",
    "    LR_results.append({'Penalty': params['penalty'], 'C': params['C'], 'Solver': params['solver'], 'Accuracy': accuracy})\n",
    "\n",
    "LR_results_df = pd.DataFrame(LR_results)\n",
    "\n",
    "# Find the row index with the maximum accuracy\n",
    "best_row_index = LR_results_df['Accuracy'].idxmax()\n",
    "best_hyperparameters = LR_results_df.loc[best_row_index, ['C', 'Solver', 'Penalty', 'Accuracy']]\n",
    "best_C = best_hyperparameters['C']\n",
    "best_solver = best_hyperparameters['Solver']\n",
    "best_penalty = best_hyperparameters['Penalty']\n",
    "\n",
    "# Fitting the model with the best hyperparameters\n",
    "LR_model = LogisticRegression(C=best_C, solver=best_solver, penalty=best_penalty)\n",
    "LR_model.fit(X_train, y_train.values.ravel())\n",
    "LR_pred = LR_model.predict(X_test)\n",
    "LR_accuracy = accuracy_score(y_test.values.ravel(), LR_pred)\n",
    "LR_cr = classification_report(y_test.values.ravel(), LR_pred)\n",
    "LR_cm = confusion_matrix(y_test.values.ravel(), LR_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### STOCHASTIC GRADIENT\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Starting Stochastic Gradient Descent...\")\n",
    "\n",
    "# Define hyperparameters for grid search\n",
    "param_grid = {\n",
    "    'alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'loss': ['hinge', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "}\n",
    "\n",
    "# Initialize SGDClassifier\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(sgd, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get best parameters and score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Fit a model using the best parameters\n",
    "SGD_model = SGDClassifier(**best_params)\n",
    "SGD_model.fit(X_train, y_train.values.ravel())\n",
    "SGD_pred = SGD_model.predict(X_test)\n",
    "SGD_accuracy = accuracy_score(y_test, SGD_pred)\n",
    "SGD_cr = classification_report(y_test.values.ravel(), SGD_pred)\n",
    "SGD_cm = confusion_matrix(y_test.values.ravel(), SGD_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### XGBOOST\n",
    "\n",
    "import xgboost as xgb\n",
    "print(\"Starting XGBoost...\")\n",
    "\n",
    "XGB_model = xgb.XGBClassifier(objective='binary:logistic', random_state=42)\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {\n",
    "    'max_depth': [3, 4, 5],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'reg_alpha': [0, 0.1, 0.5],  # L1 regularization term (alpha)\n",
    "    'reg_lambda': [0, 0.1, 0.5],  # L2 regularization term (lambda)\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=XGB_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train_LE, y_train_LE.values.ravel())\n",
    "\n",
    "# Get the best parameters and fit a model\n",
    "best_params = grid_search.best_params_\n",
    "XGB_model = xgb.XGBClassifier(**best_params)\n",
    "XGB_model.fit(X_train_LE, y_train_LE.values.ravel())\n",
    "XGB_pred = XGB_model.predict(X_test_LE)\n",
    "XGB_accuracy = accuracy_score(y_test_LE, XGB_pred)\n",
    "XGB_cr = classification_report(y_test_LE.values.ravel(), XGB_pred)\n",
    "XGB_cm = confusion_matrix(y_test_LE.values.ravel(), XGB_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### PERCEPTRON\n",
    "\n",
    "from sklearn.linear_model import Perceptron\n",
    "print(\"Starting Perceptron...\")\n",
    "\n",
    "# Define the Perceptron classifier\n",
    "perceptron_classifier = Perceptron(random_state=42)\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid = {'alpha': [0.0001, 0.001, 0.01, 0.1, 1.0]}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=perceptron_classifier, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_\n",
    "\n",
    "# Fit a model with the best params\n",
    "perceptron_classifier = Perceptron(**best_params)\n",
    "perceptron_classifier.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on test data\n",
    "perceptron_pred = perceptron_classifier.predict(X_test)\n",
    "perceptron_accuracy = accuracy_score(y_test.values.ravel(), perceptron_pred)\n",
    "perceptron_cr = classification_report(y_test.values.ravel(), perceptron_pred)\n",
    "perceptron_cm = confusion_matrix(y_test.values.ravel(), perceptron_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### GRADIENT BOOSTING\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "print(\"Starting Gradient Boosting...\")\n",
    "\n",
    "gbm = GradientBoostingClassifier()\n",
    "param_grid_gbm = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "grid_search_gbm = GridSearchCV(gbm, param_grid=param_grid_gbm, cv=5)\n",
    "grid_search_gbm.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_gbm = grid_search_gbm.best_estimator_\n",
    "best_gbm.fit(X_train, y_train.values.ravel())\n",
    "gbm_pred = best_gbm.predict(X_test)\n",
    "gbm_accuracy = accuracy_score(y_test.values.ravel(), gbm_pred)\n",
    "gbm_cr = classification_report(y_test.values.ravel(), gbm_pred)\n",
    "gbm_cm = confusion_matrix(y_test.values.ravel(), gbm_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### ADABOOST\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "print(\"Starting AdaBoost...\")\n",
    "\n",
    "adaboost = AdaBoostClassifier()\n",
    "param_grid_adaboost = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}\n",
    "\n",
    "grid_search_adaboost = GridSearchCV(adaboost, param_grid=param_grid_adaboost, cv=5)\n",
    "grid_search_adaboost.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_adaboost = grid_search_adaboost.best_estimator_\n",
    "best_adaboost.fit(X_train, y_train.values.ravel())\n",
    "adaboost_pred = best_adaboost.predict(X_test)\n",
    "adaboost_accuracy = accuracy_score(y_test.values.ravel(), adaboost_pred)\n",
    "adaboost_cr = classification_report(y_test.values.ravel(), adaboost_pred)\n",
    "adaboost_cm = confusion_matrix(y_test.values.ravel(), adaboost_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### BAGGING\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "print(\"Starting Bagging...\")\n",
    "\n",
    "bagging = BaggingClassifier()\n",
    "param_grid_bagging = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'max_samples': [0.5, 1.0],\n",
    "    'max_features': [0.5, 1.0]\n",
    "}\n",
    "\n",
    "grid_search_bagging = GridSearchCV(bagging, param_grid=param_grid_bagging, cv=5)\n",
    "grid_search_bagging.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_bagging = grid_search_bagging.best_estimator_\n",
    "best_bagging.fit(X_train, y_train.values.ravel())\n",
    "bagging_pred = best_bagging.predict(X_test)\n",
    "bagging_accuracy = accuracy_score(y_test.values.ravel(), bagging_pred)\n",
    "bagging_cr = classification_report(y_test.values.ravel(), bagging_pred)\n",
    "bagging_cm = confusion_matrix(y_test.values.ravel(), bagging_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### EXTRA TREES\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "print(\"Starting Extra Trees...\")\n",
    "\n",
    "extra_trees = ExtraTreesClassifier()\n",
    "param_grid_extra_trees = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search_extra_trees = GridSearchCV(extra_trees, param_grid=param_grid_extra_trees, cv=5)\n",
    "grid_search_extra_trees.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_extra_trees = grid_search_extra_trees.best_estimator_\n",
    "best_extra_trees.fit(X_train, y_train.values.ravel())\n",
    "extra_trees_pred = best_extra_trees.predict(X_test)\n",
    "extra_trees_accuracy = accuracy_score(y_test.values.ravel(), extra_trees_pred)\n",
    "extra_trees_cr = classification_report(y_test.values.ravel(), extra_trees_pred)\n",
    "extra_trees_cm = confusion_matrix(y_test.values.ravel(), extra_trees_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### MULTI-LAYER PERCEPTRON\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "print(\"Starting Multi-Layer Perceptron...\")\n",
    "\n",
    "mlp_classifier = MLPClassifier()\n",
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(mlp_classifier, param_grid=param_grid_mlp, cv=5)\n",
    "grid_search_mlp.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "best_mlp = grid_search_mlp.best_estimator_\n",
    "best_mlp.fit(X_train, y_train.values.ravel())\n",
    "mlp_pred = best_mlp.predict(X_test)\n",
    "mlp_accuracy = accuracy_score(y_test.values.ravel(), mlp_pred)\n",
    "mlp_cr = classification_report(y_test.values.ravel(), mlp_pred)\n",
    "mlp_cm = confusion_matrix(y_test.values.ravel(), mlp_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### GAUSSIAN PROCESS\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "print(\"Starting Gaussian Process...\")\n",
    "\n",
    "# Define the Gaussian Process Classifier\n",
    "kernel = 1.0 * RBF(1.0)\n",
    "gpc = GaussianProcessClassifier(kernel=kernel)\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid_gpc = {\n",
    "    \"max_iter_predict\": [100, 200, 300],\n",
    "    # Add more parameters to be tuned as needed for the GPC model\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search_gpc = GridSearchCV(estimator=gpc, param_grid=param_grid_gpc, cv=3, scoring='accuracy')\n",
    "grid_search_gpc.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params_gpc = grid_search_gpc.best_params_\n",
    "best_accuracy_gpc = grid_search_gpc.best_score_\n",
    "\n",
    "# Fit a model with the best params\n",
    "gpc = GaussianProcessClassifier(**best_params_gpc)\n",
    "gpc.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on test data\n",
    "gpc_pred = gpc.predict(X_test)\n",
    "gpc_accuracy = accuracy_score(y_test.values.ravel(), gpc_pred)\n",
    "gpc_cr = classification_report(y_test.values.ravel(), gpc_pred)\n",
    "gpc_cm = confusion_matrix(y_test.values.ravel(), gpc_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### QUADRATIC DISCRIMINANT\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "print(\"Starting Quadratic Discriminant...\")\n",
    "\n",
    "# Define the Quadratic Discriminant Analysis model\n",
    "qda = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid_qda = {\n",
    "    \"tol\": [0.0001, 0.001, 0.01],\n",
    "    # Add more parameters to be tuned as needed for the QDA model\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search_qda = GridSearchCV(estimator=qda, param_grid=param_grid_qda, cv=3, scoring='accuracy')\n",
    "grid_search_qda.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params_qda = grid_search_qda.best_params_\n",
    "best_accuracy_qda = grid_search_qda.best_score_\n",
    "\n",
    "# Fit a model with the best params\n",
    "qda = QuadraticDiscriminantAnalysis(**best_params_qda)\n",
    "qda.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on test data\n",
    "qda_pred = qda.predict(X_test)\n",
    "qda_accuracy = accuracy_score(y_test.values.ravel(), qda_pred)\n",
    "qda_cr = classification_report(y_test.values.ravel(), qda_pred)\n",
    "qda_cm = confusion_matrix(y_test.values.ravel(), qda_pred)\n",
    "\n",
    "##################################################\n",
    "\n",
    "##### CATBOOST\n",
    "print(\"Starting CatBoost...\")\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Define the CatBoost Classifier\n",
    "catboost = CatBoostClassifier(verbose=False)\n",
    "\n",
    "# Define the grid of parameters to search\n",
    "param_grid_catboost = {\n",
    "    \"depth\": [4, 6, 8],\n",
    "    # Add more parameters to be tuned as needed for the CatBoost model\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search_catboost = GridSearchCV(estimator=catboost, param_grid=param_grid_catboost, cv=3, scoring='accuracy')\n",
    "grid_search_catboost.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Get the best parameters and the corresponding accuracy\n",
    "best_params_catboost = grid_search_catboost.best_params_\n",
    "best_accuracy_catboost = grid_search_catboost.best_score_\n",
    "\n",
    "# Fit a model with the best params\n",
    "catboost = CatBoostClassifier(**best_params_catboost, verbose=False)\n",
    "catboost.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Make predictions on test data\n",
    "catboost_pred = catboost.predict(X_test)\n",
    "catboost_accuracy = accuracy_score(y_test.values.ravel(), catboost_pred)\n",
    "catboost_cr = classification_report(y_test.values.ravel(), catboost_pred)\n",
    "catboost_cm = confusion_matrix(y_test.values.ravel(), catboost_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Correct</th>\n",
       "      <th>Wrong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>94.357</td>\n",
       "      <td>94.409</td>\n",
       "      <td>94.357</td>\n",
       "      <td>94.369</td>\n",
       "      <td>428</td>\n",
       "      <td>291</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>719</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>94.357</td>\n",
       "      <td>94.380</td>\n",
       "      <td>94.357</td>\n",
       "      <td>94.364</td>\n",
       "      <td>430</td>\n",
       "      <td>289</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>719</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>94.226</td>\n",
       "      <td>94.307</td>\n",
       "      <td>94.226</td>\n",
       "      <td>94.242</td>\n",
       "      <td>426</td>\n",
       "      <td>292</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>718</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>94.226</td>\n",
       "      <td>94.270</td>\n",
       "      <td>94.226</td>\n",
       "      <td>94.237</td>\n",
       "      <td>428</td>\n",
       "      <td>290</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>718</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>94.094</td>\n",
       "      <td>94.187</td>\n",
       "      <td>94.094</td>\n",
       "      <td>94.113</td>\n",
       "      <td>425</td>\n",
       "      <td>292</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>717</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Machines</td>\n",
       "      <td>93.963</td>\n",
       "      <td>94.026</td>\n",
       "      <td>93.963</td>\n",
       "      <td>93.978</td>\n",
       "      <td>426</td>\n",
       "      <td>290</td>\n",
       "      <td>18</td>\n",
       "      <td>28</td>\n",
       "      <td>716</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adaboost</td>\n",
       "      <td>93.963</td>\n",
       "      <td>93.963</td>\n",
       "      <td>93.963</td>\n",
       "      <td>93.963</td>\n",
       "      <td>431</td>\n",
       "      <td>285</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>716</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>93.832</td>\n",
       "      <td>93.824</td>\n",
       "      <td>93.832</td>\n",
       "      <td>93.824</td>\n",
       "      <td>433</td>\n",
       "      <td>282</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>715</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Multi-layer Perceptron</td>\n",
       "      <td>93.832</td>\n",
       "      <td>93.926</td>\n",
       "      <td>93.832</td>\n",
       "      <td>93.851</td>\n",
       "      <td>424</td>\n",
       "      <td>291</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>715</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>93.701</td>\n",
       "      <td>93.764</td>\n",
       "      <td>93.701</td>\n",
       "      <td>93.716</td>\n",
       "      <td>425</td>\n",
       "      <td>289</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>714</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>93.701</td>\n",
       "      <td>93.719</td>\n",
       "      <td>93.701</td>\n",
       "      <td>93.707</td>\n",
       "      <td>428</td>\n",
       "      <td>286</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>714</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Gaussian Process</td>\n",
       "      <td>93.570</td>\n",
       "      <td>93.665</td>\n",
       "      <td>93.570</td>\n",
       "      <td>93.589</td>\n",
       "      <td>423</td>\n",
       "      <td>290</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>713</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bagging</td>\n",
       "      <td>93.438</td>\n",
       "      <td>93.546</td>\n",
       "      <td>93.438</td>\n",
       "      <td>93.460</td>\n",
       "      <td>422</td>\n",
       "      <td>290</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>712</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Quadratic Discriminant</td>\n",
       "      <td>93.438</td>\n",
       "      <td>93.731</td>\n",
       "      <td>93.438</td>\n",
       "      <td>93.474</td>\n",
       "      <td>416</td>\n",
       "      <td>296</td>\n",
       "      <td>12</td>\n",
       "      <td>38</td>\n",
       "      <td>712</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stochastic Gradient</td>\n",
       "      <td>92.913</td>\n",
       "      <td>93.139</td>\n",
       "      <td>92.913</td>\n",
       "      <td>92.948</td>\n",
       "      <td>416</td>\n",
       "      <td>292</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>708</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>91.995</td>\n",
       "      <td>92.076</td>\n",
       "      <td>91.995</td>\n",
       "      <td>92.016</td>\n",
       "      <td>418</td>\n",
       "      <td>283</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>701</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>90.682</td>\n",
       "      <td>90.730</td>\n",
       "      <td>90.682</td>\n",
       "      <td>90.699</td>\n",
       "      <td>415</td>\n",
       "      <td>276</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>691</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model Name  Accuracy  Precision  Recall  F1-Score   TP   TN  \\\n",
       "0                  CatBoost    94.357     94.409  94.357    94.369  428  291   \n",
       "1                   XGBoost    94.357     94.380  94.357    94.364  430  289   \n",
       "2       K-Nearest Neighbors    94.226     94.307  94.226    94.242  426  292   \n",
       "3             Random Forest    94.226     94.270  94.226    94.237  428  290   \n",
       "4       Logistic Regression    94.094     94.187  94.094    94.113  425  292   \n",
       "5   Support Vector Machines    93.963     94.026  93.963    93.978  426  290   \n",
       "6                  Adaboost    93.963     93.963  93.963    93.963  431  285   \n",
       "7             Decision Tree    93.832     93.824  93.832    93.824  433  282   \n",
       "8    Multi-layer Perceptron    93.832     93.926  93.832    93.851  424  291   \n",
       "9         Gradient Boosting    93.701     93.764  93.701    93.716  425  289   \n",
       "10              Extra Trees    93.701     93.719  93.701    93.707  428  286   \n",
       "11         Gaussian Process    93.570     93.665  93.570    93.589  423  290   \n",
       "12                  Bagging    93.438     93.546  93.438    93.460  422  290   \n",
       "13   Quadratic Discriminant    93.438     93.731  93.438    93.474  416  296   \n",
       "14      Stochastic Gradient    92.913     93.139  92.913    92.948  416  292   \n",
       "15              Naive Bayes    91.995     92.076  91.995    92.016  418  283   \n",
       "16               Perceptron    90.682     90.730  90.682    90.699  415  276   \n",
       "\n",
       "    FP  FN  Correct  Wrong  \n",
       "0   17  26      719     43  \n",
       "1   19  24      719     43  \n",
       "2   16  28      718     44  \n",
       "3   18  26      718     44  \n",
       "4   16  29      717     45  \n",
       "5   18  28      716     46  \n",
       "6   23  23      716     46  \n",
       "7   26  21      715     47  \n",
       "8   17  30      715     47  \n",
       "9   19  29      714     48  \n",
       "10  22  26      714     48  \n",
       "11  18  31      713     49  \n",
       "12  18  32      712     50  \n",
       "13  12  38      712     50  \n",
       "14  16  38      708     54  \n",
       "15  25  36      701     61  \n",
       "16  32  39      691     71  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = [\n",
    "    \"K-Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Support Vector Machines\",\n",
    "    \"Naive Bayes\", \"Logistic Regression\", \"Stochastic Gradient\", \"XGBoost\",\n",
    "    \"Perceptron\", \"Gradient Boosting\", \"Adaboost\", \"Bagging\", \"Extra Trees\",\n",
    "    \"Multi-layer Perceptron\", \"Gaussian Process\", \"Quadratic Discriminant\", \"CatBoost\"\n",
    "]\n",
    "\n",
    "accuracies = [\n",
    "    KNN_accuracy, DT_accuracy, RF_accuracy, SVM_accuracy, NB_accuracy,\n",
    "    LR_accuracy, SGD_accuracy, XGB_accuracy, perceptron_accuracy,\n",
    "    gbm_accuracy, adaboost_accuracy, bagging_accuracy, extra_trees_accuracy,\n",
    "    mlp_accuracy, gpc_accuracy, qda_accuracy, catboost_accuracy\n",
    "]\n",
    "\n",
    "classification_reports = [\n",
    "    KNN_cr, DT_cr, RF_cr, SVM_cr, NB_cr,\n",
    "    LR_cr, SGD_cr, XGB_cr, perceptron_cr,\n",
    "    gbm_cr, adaboost_cr, bagging_cr, extra_trees_cr,\n",
    "    mlp_cr, gpc_cr, qda_cr, catboost_cr\n",
    "]\n",
    "\n",
    "confusion_matrices = [\n",
    "    KNN_cm, DT_cm, RF_cm, SVM_cm, NB_cm,\n",
    "    LR_cm, SGD_cm, XGB_cm, perceptron_cm,\n",
    "    gbm_cm, adaboost_cm, bagging_cm, extra_trees_cm,\n",
    "    mlp_cm, gpc_cm, qda_cm, catboost_cm\n",
    "]\n",
    "\n",
    "predictions = [\n",
    "    KNN_pred, DT_pred, RF_pred, SVM_pred, NB_pred,\n",
    "    LR_pred, SGD_pred, XGB_pred, perceptron_pred,\n",
    "    gbm_pred, adaboost_pred, bagging_pred, extra_trees_pred,\n",
    "    mlp_pred, gpc_pred, qda_pred, catboost_pred\n",
    "]\n",
    "\n",
    "accuracy_df = pd.DataFrame({'Model Name': models, 'Accuracy': accuracies})\n",
    "accuracy_df['Accuracy'] = round(accuracy_df['Accuracy']*100, 3)\n",
    "accuracy_df = accuracy_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Confusion Matrices\n",
    "TP_list, TN_list, FP_list, FN_list = [], [], [], []\n",
    "\n",
    "for cm in confusion_matrices:\n",
    "    TP = cm[1][1] if len(cm) == 2 else 0\n",
    "    TN = cm[0][0] if len(cm) == 2 else cm[1][1]\n",
    "    FP = cm[0][1] if len(cm) == 2 else 0\n",
    "    FN = cm[1][0] if len(cm) == 2 else 0\n",
    "    \n",
    "    TP_list.append(TP)\n",
    "    TN_list.append(TN)\n",
    "    FP_list.append(FP)\n",
    "    FN_list.append(FN)\n",
    "\n",
    "# Create the DataFrame\n",
    "cm_data = {\n",
    "    'Model Name': models,\n",
    "    'TP': TP_list,\n",
    "    'TN': TN_list,\n",
    "    'FP': FP_list,\n",
    "    'FN': FN_list\n",
    "}\n",
    "\n",
    "cm_df = pd.DataFrame(cm_data)\n",
    "\n",
    "# Classification Reports\n",
    "\n",
    "# Initialize lists to store metrics\n",
    "model_names = []\n",
    "precision_list, recall_list, f1_list = [], [], []\n",
    "\n",
    "for model, prediction in zip(models, predictions):\n",
    "    if model == \"XGBoost\":\n",
    "        report = classification_report(y_test_LE.values.ravel(), prediction, output_dict=True)\n",
    "    else:\n",
    "        report = classification_report(y_test.values.ravel(), prediction, output_dict=True)\n",
    "    \n",
    "    precision = round(report['weighted avg']['precision']*100, 3)\n",
    "    recall = round(report['weighted avg']['recall']*100, 3)\n",
    "    f1 = round(report['weighted avg']['f1-score']*100, 3)\n",
    "    \n",
    "    model_names.append(model)\n",
    "    precision_list.append(precision)\n",
    "    recall_list.append(recall)\n",
    "    f1_list.append(f1)\n",
    "\n",
    "# Create the DataFrame including accuracy and ROC-AUC score\n",
    "cr_data = {\n",
    "    'Model Name': model_names,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1-Score': f1_list\n",
    "}\n",
    "\n",
    "cr_df = pd.DataFrame(cr_data)\n",
    "\n",
    "metrics = accuracy_df.merge(cr_df, on='Model Name', how='inner')\n",
    "metrics = metrics.merge(cm_df, on='Model Name', how='inner')\n",
    "metrics['Correct'] = metrics['TP'] + metrics['TN']\n",
    "metrics['Wrong'] = metrics['FP'] + metrics['FN']\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
