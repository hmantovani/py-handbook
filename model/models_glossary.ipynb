{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Glossary Notebook\n",
    "\n",
    "Welcome to the Model Glossary Notebook! This notebook is your comprehensive guide to understanding various machine learning models, their functionalities, applications, and underlying principles. As the landscape of artificial intelligence and data science continues to evolve, having a solid grasp of different models becomes increasingly vital.\n",
    "\n",
    "Within this repository, you'll find detailed explanations and insights into a wide array of models employed in the field of machine learning. From foundational algorithms to state-of-the-art techniques, each entry aims to provide a clear understanding of the model's mechanics, intended use cases, strengths, and potential limitations.\n",
    "\n",
    "For each model, expect a breakdown of its core concepts, how it operates, and practical examples illustrating its application in solving real-world problems. Whether you're a novice in the field, an experienced practitioner seeking a quick reference, or simply curious about the world of machine learning, this glossary serves as a valuable resource to expand your knowledge.\n",
    "\n",
    "Feel free to explore and delve into different sections to gain a deeper understanding of the models that shape the landscape of modern AI. With each entry, you'll uncover insights that can enhance your understanding and proficiency in leveraging these models for various tasks.\n",
    "\n",
    "We hope this notebook serves as a helpful companion on your journey through the diverse realm of machine learning models.\n",
    "\n",
    "Happy exploring!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbor (KNN)\n",
    "\n",
    "The K-Nearest Neighbor (KNN) algorithm stands as a versatile and intuitive machine learning approach used extensively for both classification and regression tasks. Its fundamental principle revolves around the notion that similar instances in a dataset often exhibit similar characteristics or behaviors.\n",
    "\n",
    "**Training Phase:** KNN, in its training phase, retains the entirety of the training dataset as a reference. This step involves no explicit training, as the algorithm doesn't derive explicit model parameters from the data. Instead, it stores instances' feature vectors for subsequent comparisons during prediction.\n",
    "\n",
    "**Prediction Process:** When making predictions with KNN, the algorithm measures the distance between a new, unseen data point and all the instances stored in the training set, typically employing distance metrics like Euclidean, Manhattan, or others, depending on the data characteristics and problem context.\n",
    "\n",
    "Upon computing distances, KNN identifies the K nearest neighbors to the input data point based on these computed distances. In classification tasks, the algorithm selects the most common class label among these neighbors as the predicted label for the input data point. For regression, it computes the average (or weighted average) of the target values of the K neighbors to predict the value for the input data point.\n",
    "\n",
    "**K Selection and Impact:** The choice of 'K' in KNN plays a pivotal role in model performance. A smaller 'K' value leads to a more flexible, potentially noisy decision boundary, whereas a larger 'K' value smoothens the decision boundaries but may obscure finer patterns present in the data.\n",
    "\n",
    "KNN's simplicity and effectiveness make it an attractive choice for various applications, especially when the underlying data exhibits discernible patterns in its structure.\n",
    "\n",
    "### Pros\n",
    "\n",
    "1. **Simple Concept**: KNN's simplicity makes it easy to understand and implement, serving as an excellent introductory algorithm for newcomers to machine learning.\n",
    "  \n",
    "2. **No Training Period**: Unlike many other algorithms, KNN doesn't require an explicit training period. It stores the entire dataset and performs computations during prediction, making it suitable for dynamic or rapidly changing data.\n",
    "\n",
    "3. **Adaptability to Data**: KNN is non-parametric, meaning it can adapt to different types of data distributions, making it useful in diverse problem domains.\n",
    "\n",
    "4. **Effective for Multiclass Classification**: It performs well in multiclass classification scenarios, where decision boundaries might be intricate.\n",
    "\n",
    "### Cons\n",
    "\n",
    "1. **Computational Intensity**: As the dataset grows larger, the computational cost of predicting new instances with KNN increases significantly because it compares the new instance against all stored instances.\n",
    "\n",
    "2. **Sensitivity to Irrelevant Features**: KNN considers all features equally important. It might perform poorly if irrelevant or noisy features are present in the dataset.\n",
    "\n",
    "3. **Optimal 'K' Value Selection**: Choosing the right 'K' value is crucial. An inappropriate 'K' may lead to overfitting or underfitting, impacting the model's performance.\n",
    "\n",
    "4. **Distance Metric Selection**: The choice of distance metric profoundly influences KNN's performance. Selecting an unsuitable metric might lead to suboptimal results.\n",
    "\n",
    "### Sources\n",
    "https://www.analyticsvidhya.com/blog/2018/03/introduction-k-neighbours-algorithm-clustering/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
