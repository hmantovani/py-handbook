{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Handbook Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix**: A confusion matrix is sometimes used to illustrate classifier performance based on the above four values (TP, FP, TN, FN). These are plotted against each other to show a confusion matrix.\n",
    "* [TP FP]\n",
    "* [TN FN]\n",
    "\n",
    "**True Positive (TP)**: A True Positive occurs when the classifier correctly identifies a sample as belonging to the positive class. For instance, a classifier correctly identifying a cat in a photo as a cat.\n",
    "\n",
    "**False Positive (FP)**: A False Positive occurs when the classifier incorrectly identifies a sample as belonging to the positive class when it actually does not. For example, a classifier wrongly classifying a dog in a photo as a cat.\n",
    "\n",
    "**True Negative (TN)**: A True Negative occurs when the classifier correctly identifies a sample as belonging to the negative class. For instance, a classifier correctly identifying a photo without any cats as not containing a cat.\n",
    "\n",
    "**False Negative (FN)**: A False Negative occurs when the classifier incorrectly identifies a sample as belonging to the negative class when it actually does not. For example, a classifier failing to identify a cat in a photo that actually contains a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these metrics let's consider the following confusion matrix:\n",
    "\n",
    "* [ 45  18 ]\n",
    "* [ 12  25 ]\n",
    "\n",
    "**Accuracy** represents the number of correctly classified data instances over the total number of data instances. Accuracy may not be a good measure if the dataset is not balanced (both negative and positive classes have different number of data instances). Consider the following scenario: There are 90 people who are healthy (negative) and 10 people who have some disease (positive). Now let’s say our machine learning model perfectly classified the 90 people as healthy but it also classified the unhealthy people as healthy. The accuracy, in this case, is 90% but this model is very poor because all the 10 people who are unhealthy are classified as healthy. By this example what we are trying to say is that accuracy is not a good metric when the data set is unbalanced. Using accuracy in such scenarios can result in misleading interpretation of results.\n",
    "\n",
    "*Accuracy = (TN + TP) / Total = (45 + 25) / 100 = 70%*\n",
    "\n",
    "**Precision** is a measure of how many of the positive predictions made are correct (true positives). \n",
    "\n",
    "*Precision = TP / (TP + FP) = 45 / (45 + 18) = 71.43%*\n",
    "\n",
    "**Recall/Sensitivity** is a measure of how many of the positive cases the classifier correctly predicted, over all the positive cases in the data. It is mostly known as Recall but sometimes also referred to as Sensitivity.\n",
    "\n",
    "*Recall = TP / (TP + FN) = 45 / (45 + 12) = 78.9%*\n",
    "\n",
    "**Specificity** is a measure of how many negative predictions made are correct (true negatives). The same as Recall but for negatives.\n",
    "\n",
    "*Specificity = TN / (TN + FP) = 25 / (25 + 18) = 58.1%*\n",
    "\n",
    "**F1-Score** is a measure combining both precision and recall. It is generally described as the harmonic mean of the two. Harmonic mean is just another way to calculate an “average” of values, generally described as more suitable for ratios (such as precision and recall) than the traditional arithmetic mean. The idea is to provide a single metric that weights the two ratios (precision and recall) in a balanced way, requiring both to have a higher value for the F1-score value to rise. For example, a Precision of 0.01 and Recall of 1.0 would give an arithmetic mean of 0.505 and a F1-score of 0.02. This is because the F1-score is much more sensitive to one of the two inputs having a low value (0.01 here). Which makes it great if you want to balance the two.\n",
    "\n",
    "*F1-Score = 2 * Precision * Recall / (Precision + Recall) = 2 * 0.7143 * 0.789 / (0.7143 + 0.789) = 74.98%*\n",
    "\n",
    "The traditional **Accuracy** is a good measure if you have quite balanced datasets and are interested in all types of outputs equally. I like to start with it in any case, as it is intuitive, and dig deeper from there as needed.\n",
    "\n",
    "**Precision** is great to focus on if you want to minimize false positives. For example, you build a spam email classifier. You want to see as little spam as possible. But you do not want to miss any important, non-spam emails. In such cases, you may wish to aim for maximizing precision.\n",
    "\n",
    "**Recall** is very important in domains such as medical (e.g., identifying cancer), where you really want to minimize the chance of missing positive cases (predicting false negatives). These are typically cases where missing a positive case has a much bigger cost than wrongly classifying something as positive.\n",
    "\n",
    "**F1-score** combines precision and recall, and works also for cases where the datasets are imbalanced as it requires both precision and recall to have a reasonable value, as demonstrated by the experiments I showed in this post. Even if you have a small number of positive cases vs negative cases, the formula will weight the metric value down if the precision or recall of the positive class is low."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
